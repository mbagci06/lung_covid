{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLab_train_lung.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyX43UYxrSwMiP2PyWyUu9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbagci06/lung_covid/blob/main/DeepLab_train_lung.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M0cTRbteyBLD"
      },
      "outputs": [],
      "source": [
        "# BASICS \n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# MODEL and TRAIN\n",
        "import torchvision\n",
        "import torchvision.models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import random\n",
        "from skimage import io\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import optim\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/mbagci06/lung_covid\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp lung_covid/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d anasmohammedtahir/covidqu"
      ],
      "metadata": {
        "id": "BCAG3N-hyGfY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7b59b6-84a4-4c79-bd5e-8d050a12e484"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lung_covid' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "covidqu.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/covidqu.zip\" -d \"/content/archive/\" &> /dev/null"
      ],
      "metadata": {
        "id": "Pu_LYKwHyy9X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run lung_covid/functions.ipynb\n",
        "%run lung_covid/DeepLab.ipynb"
      ],
      "metadata": {
        "id": "q2dcCq9hyJWa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Calling the validaton dataset\n",
        "val=Lung_Dataset_RGB(split = \"Val\",class_folder='lung',shuffle=True,sample_size=700)# The length of the validation dataset to calculate the average value of the loss and accuracy\n",
        "\n",
        "val_size=val.__len__()\n",
        "# The Train dataset \n",
        "tra=Lung_Dataset_RGB(split = \"Train\",class_folder='lung',shuffle=True,sample_size=3000)\n",
        "tra_size=tra.__len__()\n",
        "\n",
        "# Call cuda, if there is no GPU use CPU\n",
        "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# The input channel determined already because model is pretrained \n",
        "net = DeepLabv3_plus(nInputChannels=3, n_classes=2, os=8, pretrained=True).to(device=device)\n",
        "# The adam optimizer for optimization the values will be same for the other model training\n",
        "optimizer = optim.Adam(net.parameters(), lr= 0.0001)\n",
        "# The Loss function Cross entropy the value \n",
        "# BCE logistic do not need to normalization to 0 1 \n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "0JmuXiBWy6kh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=25\n",
        "# loading images and masks for mini epochs \n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "        tra, batch_size=batch_size,  num_workers=2, drop_last=True)\n",
        "\n",
        "data_loader_val = torch.utils.data.DataLoader(\n",
        "        val, batch_size=batch_size,  num_workers=2, drop_last=True)"
      ],
      "metadata": {
        "id": "GmEmfXgXzXeQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXvmKV6gze0d",
        "outputId": "df88485c-18a1-4260-f428-1a74cb6ccb4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  8 04:30:44 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    32W / 250W |   1173MiB / 16280MiB |     16%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of the epoch \n",
        "epoch_number=30\n",
        "# list for the calculating validation Loss\n",
        "val_loss_array=[]\n",
        "# list for the calculating train Loss\n",
        "train_loss_array=[]\n",
        "# list for the calculating validation accuracy\n",
        "acc_val_array=[]\n",
        "# list for the calculating train accuracy\n",
        "acc_train_array=[]\n",
        "for t in range(epoch_number):\n",
        "    # reset the loss and the accuracy values each epoch \n",
        "    val_loss=0\n",
        "    train_loss=0\n",
        "    acc_val=0\n",
        "    acc_train=0\n",
        "    for batch_idx, sample in enumerate(data_loader):\n",
        "        # Train data preparing \n",
        "        imgs , true_masks = sample['image'],sample['mask']\n",
        "        imgs = imgs.to(device=device)\n",
        "        true_masks=Negative_mask(true_masks,imgs.size(dim=0))\n",
        "        true_masks = true_masks.to(device=device)\n",
        "        net.train()\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # Train data evaluation\n",
        "            masks_pred = net(imgs)\n",
        "            loss = criterion(masks_pred, true_masks)\n",
        "            train_loss += loss.item()\n",
        "            # Updating the params\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # Acc calc train\n",
        "            acc_train+=binary_acc( masks_pred, true_masks).item()\n",
        "    for batch_idx, sample in enumerate(data_loader_val):\n",
        "        # Validation data preparing  and calling \n",
        "        # calcel gradient feature \n",
        "        net.eval()\n",
        "\n",
        "        # Call iamge and mask\n",
        "        imgs , true_masks = sample['image'],sample['mask']\n",
        "\n",
        "        imgs = imgs.to(device=device)\n",
        "        true_masks=Negative_mask(true_masks,imgs.size(dim=0))\n",
        "        true_masks = true_masks.to(device=device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            # Validation evaluation \n",
        "            masks_pred = net(imgs)\n",
        "            masks_pred = masks_pred.to(device=device)\n",
        "            loss_val = criterion(masks_pred, true_masks)\n",
        "            acc_val +=binary_acc( masks_pred, true_masks).item()\n",
        "            val_loss += loss_val.item()\n",
        "\n",
        "    batch_size\n",
        "    print('Epoch :',t+1,'/',str(epoch_number))\n",
        "    print('Validation Loss:v',(val_loss)/(val_size/batch_size))\n",
        "    print('Validation Acc:',(acc_val)/(val_size/batch_size))\n",
        "    print('Train Acc:',(acc_train)/(tra_size/batch_size))\n",
        "    print('Train  Loss   :t',(train_loss)/(tra_size/batch_size))\n",
        "    # Store epoch progress\n",
        "    val_loss_array.append((val_loss)/(val_size/batch_size))\n",
        "    train_loss_array.append((train_loss)/(tra_size/batch_size))\n",
        "    acc_val_array.append((acc_val)/(val_size/batch_size))\n",
        "    acc_train_array.append((acc_train)/(tra_size/batch_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T-JqLXaziYU",
        "outputId": "92c464aa-2b03-4918-ddcb-25e3c071622c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3704: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1 / 30\n",
            "Validation Loss:v 0.04839636332222393\n",
            "Validation Acc: 0.9829083148922239\n",
            "Train Acc: 0.9623057742913564\n",
            "Train  Loss   :t 0.09674616328751047\n",
            "Epoch : 2 / 30\n",
            "Validation Loss:v 0.03636264514976314\n",
            "Validation Acc: 0.9859997417245593\n",
            "Train Acc: 0.9850790848334631\n",
            "Train  Loss   :t 0.03960232092067599\n",
            "Epoch : 3 / 30\n",
            "Validation Loss:v 0.031821643467992544\n",
            "Validation Acc: 0.9873166765485492\n",
            "Train Acc: 0.9882786179582278\n",
            "Train  Loss   :t 0.029243632700915137\n",
            "Epoch : 4 / 30\n",
            "Validation Loss:v 0.029979209216045483\n",
            "Validation Acc: 0.9877720496484211\n",
            "Train Acc: 0.9896401156981786\n",
            "Train  Loss   :t 0.02463774544497331\n",
            "Epoch : 5 / 30\n",
            "Validation Loss:v 0.029743503845695938\n",
            "Validation Acc: 0.98771285372121\n",
            "Train Acc: 0.9898088321089744\n",
            "Train  Loss   :t 0.023278041649609803\n",
            "Epoch : 6 / 30\n",
            "Validation Loss:v 0.027708667663059065\n",
            "Validation Acc: 0.9886305183172226\n",
            "Train Acc: 0.9898110697666804\n",
            "Train  Loss   :t 0.02262801273415486\n",
            "Epoch : 7 / 30\n",
            "Validation Loss:v 0.02921659213357738\n",
            "Validation Acc: 0.9880952771220889\n",
            "Train Acc: 0.9909454291065534\n",
            "Train  Loss   :t 0.019827551604248584\n",
            "Epoch : 8 / 30\n",
            "Validation Loss:v 0.028023308808250085\n",
            "Validation Acc: 0.9882823612008776\n",
            "Train Acc: 0.9914624606569608\n",
            "Train  Loss   :t 0.018544188467785716\n",
            "Epoch : 9 / 30\n",
            "Validation Loss:v 0.02849069982767105\n",
            "Validation Acc: 0.9887206235102245\n",
            "Train Acc: 0.9918232912818591\n",
            "Train  Loss   :t 0.017511549483363826\n",
            "Epoch : 10 / 30\n",
            "Validation Loss:v 0.02981043334252068\n",
            "Validation Acc: 0.9885509993348803\n",
            "Train Acc: 0.9920826246341069\n",
            "Train  Loss   :t 0.016871550880993406\n",
            "Epoch : 11 / 30\n",
            "Validation Loss:v 0.028043514117598534\n",
            "Validation Acc: 0.9893416975225721\n",
            "Train Acc: 0.9920995901028316\n",
            "Train  Loss   :t 0.016772614714379113\n",
            "Epoch : 12 / 30\n",
            "Validation Loss:v 0.02725431090220809\n",
            "Validation Acc: 0.9897459617682866\n",
            "Train Acc: 0.9925734962026278\n",
            "Train  Loss   :t 0.01567300073802471\n",
            "Epoch : 13 / 30\n",
            "Validation Loss:v 0.02816678916237184\n",
            "Validation Acc: 0.9894161926848548\n",
            "Train Acc: 0.9927962844570478\n",
            "Train  Loss   :t 0.015191406469481687\n",
            "Epoch : 14 / 30\n",
            "Validation Loss:v 0.030331969992922887\n",
            "Validation Acc: 0.9884984599692481\n",
            "Train Acc: 0.9927772273619969\n",
            "Train  Loss   :t 0.015148117439821362\n",
            "Epoch : 15 / 30\n",
            "Validation Loss:v 0.029037622269243002\n",
            "Validation Acc: 0.9891104825905391\n",
            "Train Acc: 0.9927144313851992\n",
            "Train  Loss   :t 0.01522732238130023\n",
            "Epoch : 16 / 30\n",
            "Validation Loss:v 0.0296981608095978\n",
            "Validation Acc: 0.9891266162906375\n",
            "Train Acc: 0.9930650467673937\n",
            "Train  Loss   :t 0.014445962989702821\n",
            "Epoch : 17 / 30\n",
            "Validation Loss:v 0.0285511645488441\n",
            "Validation Acc: 0.9898181962115424\n",
            "Train Acc: 0.9931093459328015\n",
            "Train  Loss   :t 0.014342496575166782\n",
            "Epoch : 18 / 30\n",
            "Validation Loss:v 0.03205669771081635\n",
            "Validation Acc: 0.9889584992613111\n",
            "Train Acc: 0.9932503392299016\n",
            "Train  Loss   :t 0.014054708376837274\n",
            "Epoch : 19 / 30\n",
            "Validation Loss:v 0.031484338720994334\n",
            "Validation Acc: 0.989396157009261\n",
            "Train Acc: 0.9935597062110901\n",
            "Train  Loss   :t 0.013390810725589593\n",
            "Epoch : 20 / 30\n",
            "Validation Loss:v 0.03084880459521498\n",
            "Validation Acc: 0.9895924947091511\n",
            "Train Acc: 0.993963539103667\n",
            "Train  Loss   :t 0.012514781889816124\n",
            "Epoch : 21 / 30\n",
            "Validation Loss:v 0.03826973281268563\n",
            "Validation Acc: 0.9867489614656993\n",
            "Train Acc: 0.9937554856141408\n",
            "Train  Loss   :t 0.013241031548629205\n",
            "Epoch : 22 / 30\n",
            "Validation Loss:v 0.029313519863145693\n",
            "Validation Acc: 0.9895498135260173\n",
            "Train Acc: 0.993034424384435\n",
            "Train  Loss   :t 0.01459630182168136\n",
            "Epoch : 23 / 30\n",
            "Validation Loss:v 0.029827372769692113\n",
            "Validation Acc: 0.9898785714592252\n",
            "Train Acc: 0.9944244359930356\n",
            "Train  Loss   :t 0.011572090691576402\n",
            "Epoch : 24 / 30\n",
            "Validation Loss:v 0.03039861031408821\n",
            "Validation Acc: 0.9900880541120257\n",
            "Train Acc: 0.9946777428189914\n",
            "Train  Loss   :t 0.011035594049220283\n",
            "Epoch : 25 / 30\n",
            "Validation Loss:v 0.03193648166156241\n",
            "Validation Acc: 0.9895340204238892\n",
            "Train Acc: 0.9946081841985385\n",
            "Train  Loss   :t 0.011180626818289359\n",
            "Epoch : 26 / 30\n",
            "Validation Loss:v 0.031622420331197124\n",
            "Validation Acc: 0.9894392426524844\n",
            "Train Acc: 0.9943297863006592\n",
            "Train  Loss   :t 0.011722545061881343\n",
            "Epoch : 27 / 30\n",
            "Validation Loss:v 0.0307518751360476\n",
            "Validation Acc: 0.9899278432130814\n",
            "Train Acc: 0.994694355626901\n",
            "Train  Loss   :t 0.01084084043589731\n",
            "Epoch : 28 / 30\n",
            "Validation Loss:v 0.03220396442338824\n",
            "Validation Acc: 0.9898092831884112\n",
            "Train Acc: 0.9949642474452655\n",
            "Train  Loss   :t 0.010337710683234035\n",
            "Epoch : 29 / 30\n",
            "Validation Loss:v 0.03317459552947964\n",
            "Validation Acc: 0.9896366106612342\n",
            "Train Acc: 0.9949566716949145\n",
            "Train  Loss   :t 0.010378777181419234\n",
            "Epoch : 30 / 30\n",
            "Validation Loss:v 0.03594321438244411\n",
            "Validation Acc: 0.9891989891018186\n",
            "Train Acc: 0.9952671845753988\n",
            "Train  Loss   :t 0.009716461854986846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "best_model_wts = copy.deepcopy(net.state_dict())\n",
        "torch.save({    \n",
        "    'epoch':40 + 1,\n",
        "    'model_state_dict':best_model_wts,\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'best_acc': 0.18\n",
        "},os.path.join(os.getcwd(),'deeplab_lung_{}_epo.pth'.format(epoch_number)))"
      ],
      "metadata": {
        "id": "JRWSseNElVOF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save loss and acc information to the txt \n",
        "textfile = open(\"deeplab_lung_{}_epo.txt\".format(epoch_number), \"w\")\n",
        "biglist=val_loss_array+   train_loss_array+    acc_val_array+    acc_train_array\n",
        "textfile. write(\"running_loss_array+   train_loss_array+    acc_val_array+    acc_train_array\"+ \"\\n\")\n",
        "for element in [biglist]:\n",
        "    textfile. write(str(element) + \"\\n\")\n",
        "textfile. close()"
      ],
      "metadata": {
        "id": "g-mPrulmlriK"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}